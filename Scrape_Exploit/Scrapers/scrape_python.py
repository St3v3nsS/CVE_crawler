import re
import regex
from urllib.parse import unquote
from .scraper import Scraper


class PythonScraper(Scraper):
    def __init__(self, filename=None, name=None, exploit_type=None, title=None, platform=None, exploit=None, mongoclient=None, date=None):
        ext = ['.py']
        super().__init__(filename, name, exploit_type, title, platform, exploit, mongoclient, date, ext)

    def parse_infos(self):
        if self.is_parsed():
            return

        error = False
        parsed_file = True
        try:
            comments = re.findall(r'^#(.*?)(?:import)', self.exploit, flags=re.M | re.S)
            if not comments:
                comments = re.findall(r'^#(.*?)\n\n', self.exploit, flags=re.M | re.S)
            comments.extend(re.findall(r"['\"]{3}(.*?)['\"]{3}", self.exploit, flags=re.M | re.S))
            comments.extend(re.findall(r'def\s*usage(.*?)\n\n', self.exploit, flags=re.M | re.S))

            if self.check_source_at_begin():
                self.add_to("description", [self.source_at_begin[2]])
                self.add_to("targets", [self.source_at_begin[3]])

            for comment in comments:
                self.get_basic_ref(comment=comment)
                self.add_to("refs", re.findall(r'[dD]etails\s*:\s*(.*)', comment))

                list_sources = re.findall(r'References:?\n(.*)\s*', comment, flags=re.S)
                if list_sources:
                    self.add_to("refs", re.findall(r'(https?:\/\/.*)', list_sources[0]))
                else:
                    list_sources = re.findall(r'REFERENCES:?\n(.*?)\s*(?:[IXV]\.)', comment, flags=re.S)
                    if list_sources:
                        self.add_to("refs", re.findall(r'(https?:\/\/.*)', list_sources[0]))

                cvess = re.findall(r'(C[VW]Es)\s*:\s*(.*?)\n', comment)
                if cvess:
                    self.add_to("refs", [(cvess[0][0], value) for value in cvess[0][1].split(',')])

                self.add_to("name", re.findall(r'(?:Exploit\s*)?[Tt]itle\s*:\s(.*)', comment))
                self.get_basic_version(comment=comment)
                self.add_to("targets", re.findall(r'Product\s*:\s*(.*)', comment))
                self.add_to("targets", re.findall(r'Installed On\s*:\s*(.*)', comment))
                self.add_to("vversion", re.findall(r'Software\.+(.*)', comment))
                self.add_to("targets", re.findall(r'Tested [Oo]n(?:\.+|\s+:?)?(.*)', comment))
                self.add_to("description", re.findall(r'Vulnerability\s*:\s*([\s\S]*?)#\n', comment))
                self.add_to("description", re.findall(r'[Dd]escription\s*:\s*(.*)', comment))
                dash_split = re.findall(r'(?:-{8})+(.*?)(?:-{8})', comment, flags=re.S)
                for item in dash_split:
                    self.add_to("description", re.findall(r'(?:Description|Explanation)\s*:?\s*(.*)', item, flags=re.S))

                self.add_to("description", re.findall(r'Known Issues:?\n(.*?)\n\n', comment, flags=re.S))
                self.add_to("description", re.findall(r'(?:VULNERABILITY|INTRODUCTION)\n(.*?)\s*[IVX]\.', comment, flags=re.S))

            if not self.extracted_object["description"]:
                self.add_to("description", re.findall(r'description=[\'\"](.*)[\'\"]', self.exploit))
            if not self.extracted_object["refs"]:
                self.add_to("refs", re.findall(r'(C[VW]E)(?:\s*[-:]\s*)?((?:\d+)?-\d+)', self.exploit))
                list_sources = re.findall(r'References:?\n(.*)\s*', self.exploit, flags=re.S)
                if list_sources:
                    self.add_to("refs", re.findall(r'(https?:\/\/.*)', list_sources[0]))

            URI = self.parse_url()
            self.update_db(URI)

        except Exception as e:
            error, parsed_file = self.founded_error(e)

        finally:
            self.update_parsed_obj_db(parsed_file, error)

    def parse_url(self):
        URIs = []

        try:
            URIs.extend(regex.findall(r'(https?://.*\/.*?)[\)\"]', self.exploit, timeout=5))
        except TimeoutError: pass
        try:
            URIs.extend(self.get_basic_url())
        except TimeoutError:
            pass
        try:
            URIs.extend(regex.findall(r'(?:url|path)\s*[=:]\s*[\'\"](.*?)[\'\"]', self.exploit, timeout=5))
        except TimeoutError: pass
        try:
            URIs.extend(self.get_method_url())
        except TimeoutError:
            pass
        try:
            array = regex.findall(r'paths?\s*=\s*\[?(.*)\]?', self.exploit, timeout=5)
            if array:
                array = array[0]
                if ',' in array:
                    array = array.split(',')
                    URIs.extend(['/' + elem.lstrip('/') for elem in array])
                else:
                    URIs.extend(['/'+array])
        except TimeoutError: pass
        try:
            URIs.extend(regex.findall(r'(?:req|resp)\s*=\s*.*[\"\'](.*?)[\"\']', self.exploit, timeout=5))
        except TimeoutError: pass
        try:
            urls = regex.findall(r'request\(.*?,.*?\s*[\'\"](.*?)[\"\']', self.exploit, timeout=5)
            URIs.extend([url.lstrip('%s') for url in urls])
        except TimeoutError: pass
        try:
            URIs.extend(regex.findall(r'(?:Request|urlopen)\(.*?[\'\"](.*?)[\'\"].*?\)', self.exploit, timeout=5))
        except TimeoutError: pass
        try:
            URIs.extend(regex.findall(r'requests?\..*\(.*?\+.*?[\'\"](.*?)[\"\'].*\)?', self.exploit, timeout=5))
        except TimeoutError: pass
        try:
            urls = regex.findall(r'(?:GET|POST|PUT|PATCH|DELETE)\s*(.*?)\s*\?', self.exploit, timeout=5)
            if urls:
                URIs.extend([unquote(url) for url in urls])
        except TimeoutError: pass

        return self.extract_url(URIs)
