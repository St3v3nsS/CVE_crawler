from .scraper import Scraper
import re
import json
import regex


class PerlScraper(Scraper):
    def __init__(self, filename=None, name=None, exploit_type=None, title=None, platform=None, exploit=None, mongoclient=None, date=None):
        ext = ['.pl', '.pm']
        super().__init__(filename, name, exploit_type, title, platform, exploit, mongoclient, date, ext)

    def parse_infos(self):
        cves = self.db["cves"]
        if self.is_parsed():
            return

        error = False
        parsed_file = True
        try:
            title = self.construct_title()
            dictionary = None
            if re.findall('Msf', self.exploit):
                dictionary = self.parse_metasploit()

            if self.check_source_at_begin():
                if ('#' not in self.source_at_begin[2] or '#' not in self.source_at_begin[2])or 'use' not in self.source_at_begin[2]\
                        or 'my' not in self.source_at_begin[2]:
                    self.add_to("description", [self.source_at_begin[2]])
                if len(self.source_at_begin[1]) > 2 and '#' not in self.source_at_begin[3] or 'use' not in self.source_at_begin[3]\
                        or 'my' not in self.source_at_begin[3]:
                    self.add_to("targets", [self.source_at_begin[3]])

            self.add_to("refs", re.findall(r'(?:based on|[sS]ee|[vV]isit|[pP]ublished at|[Mm]ore)\s+:?\s*(.*?)\s', self.exploit))

            self.get_basic_ref()
            comments = re.findall(r'^(#.*?)(?:package|use|\$|my)', self.exploit, flags=re.S | re.M)
            comments.extend(re.findall(r'\s\/\/(.*)', self.exploit))

            for comment in comments:
                self.get_basic_name(comment=comment)
                self.get_basic_version(comment=comment)
                self.add_to("description", re.findall(r'(?:Desc(?:ription)?|Summary)\s*:?\s*(.*?)\n\n', comment, flags=re.S | re.M))
                self.add_to("refs", re.findall(r'References?:?\s*(.*?)\s', comment))
                self.add_to("refs", re.findall(r'(?:Software [lL]ink\s*|advisor(?:y|ies)):\s*(.*)', comment))
                self.add_to("targets", re.findall(r'[Tt]ested\s*(?:on|with)\s*:?\s*(.*)', comment))

            references = []

            URI = self.parse_url()

            if dictionary is not None:
                self.add_to("name", [dictionary.get('Name')])
                self.add_to("description", [dictionary.get('Description')])
                if dictionary.get('Version') is not None:
                    self.add_to("vversion", ([dictionary.get('Version')]))
                if dictionary.get('Refs') is not None:
                    references.extend(dictionary.get('Refs'))
                if dictionary.get('Opts').get('RPATH') is not None and len(dictionary.get('Opts').get('RPATH')) > 3:
                    URI.append(dictionary.get('Opts').get('RPATH')[3])

            for ref in list(set(self.extracted_object["refs"])):
                if isinstance(ref, tuple):
                    references.append([ref[0], ref[1]])
                else:
                    references.append(['URL', ref])

            description = self.get_description()
            myDict = self.create_object_for_mongo(title, description, references, URI)

            cves.update({"EDB-ID": self.name}, myDict, upsert=True)

        except Exception as e:
            error, parsed_file = self.found_error(e)
            
        finally:
            self.update_parsed_obj_db(parsed_file, error)

    def parse_url(self):
        URIs = []

        self.exploit = re.sub(r'\$ARGV\[\d\]', '/', self.exploit)

        try:
            URIs.extend(self.get_basic_url())
        except TimeoutError:
            pass
        try:
            URIs.extend(self.get_method_url())
        except TimeoutError:
            pass
        try:
            URIs.extend(regex.findall(r'(\/[\/.a-zA-Z0-9-_]+)', self.exploit, timeout=5))
        except TimeoutError: pass
        try:
            URIs.extend([''.join(re.findall(r'(?:path|url)\s*\.?=\s*\"?(.*)\"?;\s+', self.exploit))])
        except TimeoutError: pass

        return self.extract_url(URIs)

    def parse_metasploit(self):
        info = re.findall(r'my \$info\s*=\s*({.*?});\s+sub', self.exploit, flags=re.S)

        if not info:
            return None
        info = info[0]
        info = re.sub(r'=>', ':', info)
        info = re.sub(r'(\s#.*)', '', info)
        description = re.findall(r"[\"\']Description[\"\']\s*:\s*(.*?),\s*[\"\']", info, flags=re.S)[0]
        description = re.sub(r'"', "'", description)
        description = re.sub(r'\s+', ' ', description)
        description = re.sub(r'\\', '\\\\', description)
        info = re.sub(r"'", '"', info)
        info = re.sub(r'\s+', ' ', info)
        info = re.sub(r'r,\s*([}\)\]])', r'\g<1>', info)
        info = re.sub(r'\\\$', '', info)
        info = re.sub(r'((?:BadChars|Space)\"\s*:)\s*.*?([,}\]]\s*[,\"\[{}\]])', r'\g<1> ""\g<2>', info)
        info = re.sub(r'(Prepend(?:Encoder)?\"\s*:)\s*.*?([,}\]]\s*[,\"\[{}\]])', r'\g<1> ""\g<2>', info)
        info = re.sub(r'0x[a-fA-F0-9]+', r'""', info)

        info = re.sub(r'\((\d+\+\d+)+\)', r'""', info)
        info = re.sub(r'\"\"\s*\+\s*\d+', r'""', info)
        info = re.sub(r'\\&.*?\s', '""', info)
        info = re.sub(r'([\"\']Description\"\s*:\s*).*?(,\s*[\"\'])', r'\g<1>{}\g<2>'.format(description.replace('\\', '\\\\')), info, flags=re.S)

        info = re.sub(r'(?:Pex::Text::Freeform\s*\(\s*)?qq?\s*{(.*?)}\s*\)?,', r'"\g<1>", ', info, flags=re.S)
        try:
            info = json.loads(info.replace('\\', '\\\\'))
            mydict = {
                'Name': info.get('Name'),
                'Description': info.get('Description'),
                'Version': info.get('Version'),
                'Refs': info.get('Refs'),
                'Targets': info.get('Targets'),
                'Opts': info.get('UserOpts')
            }
            return mydict
        except Exception as e:
            self.logger.error(self.filename + f'\t{str(e)}')
            self.logger.warning(self.filename + f'\t{info}')
            return None
