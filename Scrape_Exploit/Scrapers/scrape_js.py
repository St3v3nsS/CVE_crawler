import re
import regex
from .scraper import Scraper
from six import string_types


class JSParser(Scraper):
    def __init__(self, filename=None, name=None, exploit_type=None, title=None, platform=None, exploit=None, mongoclient=None, date=None):
        ext = ['.js', '.svg']
        super().__init__(filename, name, exploit_type, title, platform, exploit, mongoclient, date, ext)

    def parse_infos(self):
        if self.is_parsed():
            return

        error = False
        parsed_file = True
        try:
            new_comments = []

            # Even if the name of exploit exists, I need it for finding other stuffs, but it will not be included in the json

            comments = re.findall(r'\/\*(.*?)\*\/', self.exploit, flags=re.S | re.M)  # Multiline comments
            comments.extend(re.findall(r'^//(.*)', self.exploit))  # Single-line comments

            if self.check_source_at_begin():
                if ('###' not in self.source_at_begin[2] or '===' not in self.source_at_begin[2]):
                    self.add_to("description", [self.source_at_begin[2]])
                if len(self.source_at_begin[1]) > 2 and '####' not in self.source_at_begin[3]:
                    self.add_to("targets", [self.source_at_begin[3]])

            for array in comments:  # Make array from array of arrays
                if isinstance(array, string_types):
                    new_comments.append(array)
                else:
                    for comment in array:
                        new_comments.append(comment)

            for comment in new_comments:

                # All posibilities depending on how they write their code
                self.add_to("refs", re.findall(r'(https?://[^,\'\s\"\]\)]+)', comment))
                self.get_basic_ref(comment=comment)
                self.add_to("description", 
                    re.findall(r'(?:Description|Summary|Product|DESCRIPTION)\s*:?\s*(.*)\w', comment, flags=re.M))
                if '* ' in comment and not self.extracted_object["description"]:
                    value = re.findall(r'\*\s*(.*)', comment)
                    if value and '***' not in value[0]:
                        self.add_to("name", [value[0]])
                        if len(value) > 1:
                            self.add_to("description", [value[1]])

                if not self.extracted_object["description"] and '***' in comment:
                    value = re.findall(r'\n(.*)', comment)
                    if value and len(value) > 1:
                        self.add_to("description", [value[1]])

                self.add_to("vversion", re.findall(r'Vulnerable version\s*:\s*(.*)', comment))
                self.add_to("vversion", 
                    re.findall(r'[^\w/](?:VERSIONS?|Versions?(?:\s*numbers:?\s*-+\n)?)\s*:?\s*(.*\s+.*)', comment))
                self.add_to("vversion", re.findall(r'Affected\s*version\s*:\s*(.*\s*.*)?\s*\w+:', comment))
                self.add_to("name", re.findall(r'[^\w/](?:Title|Name|Topic|Software)\s*:?\s*(.*)', comment))
                self.add_to("targets", re.findall(r'(?:Tested|TESTED)\s*(?:on|ON)\s*:\s*(.*)', comment))
            
            URI = self.parse_url()
            self.update_db(URI)
        except Exception as e:
            error, parsed_file = self.founded_error(e)
        finally:
            self.update_parsed_obj_db(parsed_file, error)

    def parse_url(self):
        URIs = []

        try:
            URIs.extend(regex.findall(r'(https?://.*\/.*?)[\)\"]', self.exploit, timeout=5))
        except TimeoutError: pass
        try:
            URIs.extend(self.get_basic_url())
        except TimeoutError:
            pass
        try:
            URIs.extend(regex.findall(r'(?:url|path)\s*[=:]\s*[\'\"](.*?)[\'\"]', self.exploit, timeout=5))
        except TimeoutError: pass
        try:
            URIs.extend(self.get_method_url())
        except TimeoutError:
            pass
        try:
            construct_uri = regex.findall(r'action=\s*.*?document.*?\+(.*?)\+(.*?)\+(.*?);', self.exploit, timeout=5)
            for uri_to_construct in construct_uri:
                value1 = re.findall(re.escape(uri_to_construct[0]), self.exploit)[0]
                value2 = re.findall(re.escape(uri_to_construct[1]), self.exploit)[0]
                value3 = re.findall(re.escape(uri_to_construct[2]), self.exploit)[0]

                URIs.extend([value1 + value2 + value3])
        except TimeoutError: pass

        return self.extract_url(URIs)
