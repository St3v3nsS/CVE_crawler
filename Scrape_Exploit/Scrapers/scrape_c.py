import re
import regex
from .scraper import Scraper


class CScraper(Scraper):
    def __init__(self, filename=None, name=None, exploit_type=None, title=None, platform=None, exploit=None, mongoclient=None, date=None):
        ext = ['.c', '.cpp', '.cs', '.m', '.cc']
        super().__init__(filename, name, exploit_type, title, platform, exploit, mongoclient, date, ext)

    def parse_infos(self):
        if self.is_parsed():
            return

        error = False
        parsed_file = True
        try:

            if self.check_source_at_begin():
                if ('###' not in self.source_at_begin[2] or '#include' not in self.source_at_begin[2]):
                    self.add_to("description", [self.source_at_begin[2]])
                if len(self.source_at_begin[1]) > 2 and '#include' not in self.source_at_begin[3] or '*/' not in self.source_at_begin[3]\
                        or '//' not in self.source_at_begin[3]:
                    self.add_to("targets", [self.source_at_begin[3]])

            comments = re.findall(r'^\/\*(.*?)\*\/', self.exploit, flags=re.S | re.M)
            comments.extend(re.findall(r'\s\/\/(.*)', self.exploit))

            for comment in comments:
                self.get_basic_name(comment=comment)
                self.get_basic_version(comment=comment)
                self.add_to("description", re.findall(r'Description\s*(?:-)*\n(.*?)\n(?:\n|\s*\*\s*-)', comment, flags=re.S | re.M))
                self.add_to("description", re.findall(r'(?:Desc(?:ription)?|Summary)\s*:\s*(.*?)\|\n\|\s*\|', comment, flags=re.S | re.M))
                self.add_to("description", re.findall(r'DESCRIPTION\s+(.*?)[IVX]', comment, flags=re.S))
                self.get_basic_ref(comment=comment)
                self.add_to("refs", re.findall(r'(https?:.*)\s*', comment))
                self.add_to("targets", re.findall(r'[Tt]ested\s*(?:on|with)\s*:?\s*(.*)', comment))

            URI = self.parse_url()
            self.update_db(URI)

        except Exception as e:
            error, parsed_file = self.founded_error(e)

        finally:
            self.update_parsed_obj_db(parsed_file, error)

    def parse_url(self):
        URIs = []

        try:
            URIs.extend(self.get_basic_url())
        except TimeoutError:
            pass
        try:
            URIs.extend(self.get_method_url())
        except TimeoutError:
            pass
        try:
            URIs.extend(regex.findall(r'".*?(\/.*?)[\s"]', self.exploit, timeout=5))
        except TimeoutError: pass
        try:
            URIs.extend(regex.findall(r'#define\s.*?"(.*?)["\s]', self.exploit, timeout=5))
        except TimeoutError: pass
        try:
            URIs.extend(regex.findall(r'http:\/\/.*?(\/.*?)[\s\\)\]"]', self.exploit, timeout=5))
        except TimeoutError: pass

        return self.extract_url(URIs)
